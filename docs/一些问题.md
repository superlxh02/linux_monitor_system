## 1. 我看你项目里用 eBPF，你了解 eBPF 吗？

**面试回答示例：**

eBPF 可以理解为“在内核里运行的、可验证的轻量程序机制”。

我对 eBPF 的理解可以概括成三点：

- **能力定位**：它让我们在不改内核源码的前提下，把观测/过滤/统计逻辑挂到内核关键路径；
- **安全机制**：程序加载前会经过 verifier 校验，限制循环、指针访问和执行路径，避免破坏内核稳定性；
- **数据交换**：通过 BPF map 在内核态和用户态之间共享统计数据，用户态只做读取与聚合。

它典型用于：网络流量观测、系统调用追踪、性能剖析、延迟热点定位等。

一句话：eBPF 的核心价值是**高实时、低侵入、可编程**。

---

## 2.说说 eBPF 在你项目里是怎么用的

**面试回答示例：**

在我的项目里，我主要用 eBPF 做**网络流量监控**：

- 在 Worker 侧，把 BPF 程序挂到网卡 TC 的 ingress/egress；
- 在内核里按 ifindex 统计收发字节和包数（hash map）；
- 用户态周期读取 map，用“当前值-上次值”做差分，算出 bytes/s 和 packets/s；
- 再把每块网卡的速率写进 MonitorInfo，通过 gRPC 推送给 Manager。

这样做的好处是：统计在内核完成，精度和实时性好，用户态只做轻量读取和聚合，对业务进程影响小。

---

## 3. 为什么你选择用 eBPF 做网络流量采集？

**面试回答示例：**

我做过 3 种方案对比，最后选 eBPF（TC 方案）：

### 方案一：直接读取 `/proc`（如 `/proc/net/dev`）

- 优点：实现最简单，兼容性好；
- 问题：采样粒度和更新路径受限，**有感知延迟**，高峰突发流量不够灵敏；
- 结论：适合基础监控，不适合我这个“需要更实时网络速率”的场景。

### 方案二：自定义内核模块采集网络流量

- 优点：理论上可以拿到内核路径上的数据；
- 问题：开发维护成本高、版本适配成本高、稳定性与运维复杂度都高；
- 关键点：内核模块更适合像 CPU/softirq 这种结构化统计暴露，
  **不太适合网络流量这种实时性要求高且路径变化快的数据采集场景**。

### 方案三：eBPF 网络采集（最终方案）

- 优点：低侵入、实时性好、维护成本相对可控；
- 通过 map 做累计计数，用户态做差分即可得到速率；
- 与当前 Manager/Worker 架构结合自然，能稳定推送 per-NIC 数据。

进一步在 eBPF 网络技术里，我比较了 **XDP** 和 **TC**：

- **XDP**：挂在驱动非常早期路径，性能极高，但更偏极致包处理/丢包/转发场景；
  在本项目里我们主要做监控统计，不需要这么“前置”和激进的处理能力。
- **TC**：语义更贴近收发统计，工程可控性更好，便于按网卡 ingress/egress 统一采集。

因此最终选择 **TC hook**，并挂在：

- `tc/ingress`：统计入方向（接收）
- `tc/egress`：统计出方向（发送）

也就是把程序挂在每块网卡的 ingress/egress 路径上，按 `ifindex` 统计 `bytes/packets`，再由用户态周期读取 map 计算速率。

---

## 4. 我看你自定义了内核模块，你自定义了哪些内核模块，为什么要这些自定义内核模块

**面试回答示例：**

我自定义了两个内核模块：

- `cpu_stat_collector`：采集每个 CPU 的 user/system/idle/iowait/irq/softirq 等累计时间；
- `softirq_collector`：采集每个 CPU 各类 softirq（timer/net_rx/net_tx/rcu 等）的累计计数。

做这两个模块的原因主要有三点：

- **精细度要求高**：我们需要 per-CPU、细分项的数据，不只是主机总 CPU 利用率；
- **低开销高频采样**：模块内用 hrtimer 周期更新，共享内存给用户态 mmap 读取，开销小；
- **和评分/排障强相关**：CPU 状态和 softirq 在高并发场景非常关键，能区分是计算瓶颈、I/O 等待还是网络中断压力。

---

## 5. 你内核模块怎么设计和实现的

**面试回答示例：**

我的设计思路是“**内核定时采集 + 字符设备 mmap 暴露 + 用户态差分计算**”：

1. **内核态采集**

- 用 hrtimer（1 秒周期）在内核里更新统计数组；
- `cpu_stat_collector` 读 `kcpustat_cpu` 等 CPU 时间字段；
- `softirq_collector` 读 `kstat_softirqs_cpu` 各类软中断计数。

2. **数据暴露方式**

- 每个模块注册字符设备（例如 `/dev/cpu_stat_monitor`、`/dev/cpu_softirq_monitor`）；
- 通过 `remap_pfn_range` 把内核缓冲区映射到用户态，用户态可直接 mmap 读取。

3. **用户态处理**

- Worker 的 monitor 组件打开设备并 mmap；
- 对累计值做两次采样差分，计算利用率/每秒速率；
- 填充到 protobuf 的 `MonitorInfo`，统一推送。

4. **稳定性处理**

- 如果设备节点不存在，用户态会降级并做节流重试，避免频繁报错；
- 模块卸载时释放设备、定时器和内存，保证生命周期完整。

这个方案相比每次解析 `/proc`，在高频采集下更稳定，也更容易拿到 per-CPU 细节。

---

## 6. 你 gRPC 的通信怎么设计的

**面试回答示例：**

我采用的是**推送式架构**：Worker 主动推，Manager 统一接。

- **上行链路（采集数据）**

  - Worker 周期采集所有指标，组装成一个 `MonitorInfo`；
  - 调用 Manager 的 `SetMonitorInfo` 上报；
  - Manager 收到后做两件事：写内存缓存 + 回调 HostManager 落 MySQL并计算健康分。
- **下行链路（查询数据）**

  - Manager 对外提供独立 `QueryService`；
  - 支持按时间范围、分页、阈值、排序查询性能、趋势、异常、网络/磁盘/内存/softirq 明细等。
- **为什么这么设计**

  - 多节点场景下，Manager 不需要维护轮询任务，扩容简单；
  - protobuf 强约束字段，接口演进可控；
  - 读写职责分离：写入链路（HostManager）和查询链路（QueryManager）解耦，后续优化空间大。

---

## 7. 你设计了哪些性能采集指标，这些指标的意义是什么，我拿到这些数据怎么分析

**面试回答示例：**

我把指标分成 5 大类：

1. **CPU 类**

- 总体使用率、user/system/idle/iowait/irq/softirq；
- 意义：判断是计算饱和、内核态压力、还是 I/O 等待。

2. **负载类**

- `load_avg_1/5/15`；
- 意义：看任务排队趋势，补充 CPU 利用率不能反映的“排队压力”。

3. **内存类**

- used_percent、total/free/avail、buffers/cached/active/inactive 等；
- 意义：判断是否存在内存紧张、回收压力、缓存命中变化。

4. **磁盘类**

- 各盘 util%、IOPS、吞吐、平均读写时延；
- 意义：识别磁盘是否成为延迟瓶颈。

5. **网络与软中断类**

- 网卡收发速率/包速率（eBPF）、softirq 各类每秒速率；
- 意义：判断网络流量与中断处理是否导致 CPU 抢占或抖动。

**分析方法（面试可直接说流程）：**

- 先看**趋势**：CPU、内存、磁盘 util、网络速率是否同步上升；
- 再看**关联**：例如 `softirq/net_rx` 飙升 + system/softirq 比例上升，通常是网络压力；
- 再看**变化率**：突增比绝对值更能发现异常；
- 最后做**分层定位**：主机级异常 → 细化到网卡/磁盘/CPU 核心级明细。

一句话总结：我不是只看单点阈值，而是看“趋势 + 关联 + 变化率”。

---

## 8. 你的健康评分是个什么算法，为什么这样设计

**面试回答示例：**

我的健康评分是一个**可切换场景的加权线性模型**，目标是快速比较“哪台机器更适合接新流量”。

- 先把每项指标归一化为 0~1 的“健康分”（越空闲越高）：

  - CPU/内存/磁盘：`1 - usage/100`；
  - 负载：`1 - load1/(cpu_cores*1.5)`；
  - 网络：以 1Gbps 作为参考上限，对收发速率做归一化后取平均。
- 再做加权汇总（项目里的权重）：

  - `BALANCED`：CPU 35%、内存 30%、负载 15%、磁盘 15%、网络 5%
  - `HIGH_CONCURRENCY`：CPU 45%、内存 25%、负载 15%、磁盘 10%、网络 5%
  - `IO_INTENSIVE`：CPU 20%、内存 15%、负载 20%、磁盘 35%、网络 10%
  - `MEMORY_SENSITIVE`：CPU 20%、内存 45%、负载 15%、磁盘 10%、网络 10%

  其中负载归一化中的系数 N 也会跟场景变化（如高并发取 1.2、I/O 密集取 2.0）。
- 最终映射到 0~100 分。

**为什么这样设计：**

- 不同业务瓶颈不同：有的吃 CPU，有的吃磁盘，有的吃内存，单一权重会失真；
- 通过场景化权重，评分排序更贴近真实业务压力；
- 保留统一公式，保证可解释性和实现简单性。

这个模型优点是简单、可解释、计算快，适合实时选优；并且可以通过 `scoring_profile` 在查询与面板中按场景切换。后续也可以升级成“按历史回归动态调权”。

---
